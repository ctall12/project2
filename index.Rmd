---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

Cade Talley (cct847)

### Introduction 

This dataset called "Medicaid" was found from one of the databases provided, and details cross-section data from a 1986 Medicaid Consumer Survey of California residents. Some of the variables included were age, income ratio, access to healthcare ratio, exposure (length of observation for ambulatory care), number of doctor visits, health status principal component and enrollment in a managed care demonstration program. My primary interest in this dataset was to analyze some of the factors among those enrolled in a Medicaid program, while utilizing the binary "enroll" variable to find statistical differences in health identifiers between individuals enrolled in a demonstration program versus those who were not. 

```{R}
library(tidyverse)
library(readr)
Medicaid <- read_csv("Medicaid1986.csv")
# read your datasets in here, e.g., with read_csv()

# if your dataset needs tidying, do so here
Medicaid <- Medicaid %>% select(c(-"program", -"married", -"ethnicity", -"school", -"exposure", -"X1", -"health2"))


# any other code here
```

### Cluster Analysis

```{R}
library(cluster)
library(ggplot2)
library(GGally)
# clustering code here

dassit <- Medicaid %>% dplyr::select(health1, access, visits) %>% select(where(is.numeric)) %>% scale
dassit <- as.data.frame(dassit)

sil_width <- vector()
for(i in 2:10){
  pam_fit <- pam(dassit, k=i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}

ggplot()+geom_line(aes(x=1:10, y = sil_width)) + scale_x_continuous(name = "k", breaks = 1:10)

#dassit$silinfo$avg.width
pam1 <- dassit %>% pam(k=2)
pam1
pam1$silinfo$avg.width

##ggpairs plot with clusters 
pamclust <- dassit %>% select(health1, access, visits) %>% mutate(cluster=as.factor(pam1$clustering)) 

ggpairs(pamclust, columns=1:4, aes(color=cluster))

```

To identify which k-value best fit this data, a silhouette plot was made which identified the highest average sil_width association with the k-value of 2. Using this, the highest sil_width average was identified as 0.3177151. The three variables selected for clustering were health1(health identifier ratio), access to care and number of visits. Since the average silhouette width can be interpreted as a weak structure, the clustering data follows this weak/potentially artificial trend. Across access and health1, the corr value was reported as -.079 (minimal to no correlation). Similarly, across access and visits, the correlation value was recorded as 0.029. The strongest correlation within this comparison was between visits and health1, which recorded a correlation value of 0.269. 
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here -- easier using `princomp(..., cor=T)`

#Perform PCA on at least three of your numeric variables (3 is the bare minimum: using more/all of them will make this much more interesting)! You can use `eigen()` on the correlation matrix, but `princomp(..., cor=T)` is probably going to be easier.

pca1 <- princomp(dassit, center = T, scale = T)
summary(pca1, loadings = T)

eigval<-pca1$sdev^2 
varprop=round(eigval/sum(eigval), 2) 

ggplot() + geom_bar(aes(y=varprop, x=1:3), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:3)) +
  geom_text(aes(x=1:3, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) +
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) +
  scale_x_continuous(breaks=1:3)

#This cum proportion of variants led me to choose PC1/PC2, since PC3 is greater than 0.8
round(cumsum(eigval)/sum(eigval), 2)

#Visualize the observations' PC scores for the PCs you retain (keep at least PC1 and PC2) in ggplot. A biplot with `fviz_pca()` is fine too!
    
plotdf <- data.frame(PC1 = pca1$scores[,1], PC2= pca1$scores[,2])
ggplot(plotdf, aes(PC1, PC2)) +geom_point()

```

A PCA was ran on the dataset used in the previous section, using the variables health1, access and visits. This returned the comp. values of  0.42 (comp. 1), 0.76 (comp. 2), and 1.00 (comp. 3). To identify which comp/loadings to use, I constructed a ggplot to identify a potential elbow/break in the graph. Unfortunately, the values showed a relatively linear decrease, so PC1 and PC2 were retained as PC 3's value was greater than 0.80. In PC1, access has a negative sign while health1 and visits have positive signs. The two highest (strongest) loadings recorded were in PC1/PC2, the highest being the access variable for PC2 at -0.957. Following the latter, the health1 variable in PC1 was recorded as 0.714. However, PC2 only contained loadings for the variables access and visits. In PC2, all the loadings have similar signs but differ greatly in magnitude. Taking the absolute value of these negative values would show that the access variable was stronger overall (at -0.957) compared to the visits variable (-0.275). 


###  Linear Classifier

```{R}
# linear classifier code here
#Converting enroll variable to binary "1" (yes) or "0" (no)
Medicaid <- Medicaid %>% mutate(enrollbi = ifelse(enroll=="yes",1,0))

#Using a linear classifer, (e.g., linear regression, logistic regression, SVM), predict a binary variable (response) from ALL of the rest of the numeric variables in your dataset (if you have 10+, OK to just pick 10).
library(glmnet)
Medicaid <- Medicaid %>% select(c(-"gender", -"enroll"))

#logistic regression (linear classifier)
linclass <- glm(enrollbi~., data=Medicaid, family = "binomial")


#Train the model to the entire dataset and then use it to get predictions for all observations. Run the `class_diag` function or equivalent to get in-sample performance and interpret, including a discussion of how well the model is doing per AUC. Finally, report a confusion matrix.
train_probs <- predict(linclass, type = "response")
class_diag(train_probs, Medicaid$enrollbi, positive = "1")

#confusion matrix
y_hat <- ifelse(train_probs>.5, 1, 0)

table(actual = Medicaid$enrollbi, predicted = y_hat) %>% addmargins
```

```{R}
# cross-validation of linear classifier here

#Perform k-fold CV on this same model (fine to use caret). Run the `class_diag` function or equivalent to get out-of-sample performance averaged across your k folds and discuss how well is your model predicting new observations per CV AUC
library(caret)
k = 10

data <- Medicaid[sample(nrow(Medicaid)), ]
folds <- cut(seq(1:nrow(Medicaid)), breaks= k, labels= F) 

diags <- NULL
for (i in 1:k) {
    #creates training and test sets
    train <- data[folds != i,]
    test <- data[folds == i,]
    truth <- test$enrollbi
    
    # test model on test set
    fit <- glm(enrollbi ~ ., data= train, family= "binomial")
    probs <- predict(fit, newdata= test, type= "response")
    
    #diagnostics for fold
    diags <- rbind(diags, class_diag(probs, truth, positive = "1"))
}

summarize_all(diags, mean)

```

The accuracy of my logistic regression was 0.6175, meaning that proportion of participants in the dataset were correctly classified as enrolled in a demonstration program through Medicaid based on the variables visits, children, age, health1, and access. The sensitivity was calculated to be 0.5863, meaning roughly 58% of the positive reports from the data were actually positive. This model is not predicting new observations well per CV AUC as the value was calculated at 0.6211, as well as showing signs of overfitting due to poor CV performace. Additionally, a confusion matrix reports a table of true and actual values (using "0" and "1"). 


### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here

#Fit a non-parametric classifier (e.g., k-nearest-neighbors, classification tree) to the exact same dataset/variables you used with the linear classifier (same response variable too).
knn_fit <- knn3(factor(enrollbi ==1, levels=c("TRUE", "FALSE")) ~ visits+children+age+income+health1+access, data = Medicaid, k =5)
y_hat_knn <- predict(knn_fit, Medicaid)
head(y_hat_knn)

table(truth = factor(Medicaid$enrollbi==1, levels =c("TRUE", "FALSE")), prediction = factor(y_hat_knn[,1]>.5, levels = c("TRUE", "FALSE")))
#Train the model to the entire dataset and then use it to get predictions for all observations. Run the `class_diag` function or equivalent to get in-sample performance and interpret, including a discussion of how well the model is doing per AUC. Finally, report a confusion matrix.

class_diag(y_hat_knn[,1],Medicaid$enrollbi, positive= "1")

```

```{R}
# cross-validation of np classifier here

#Perform k-fold CV on this same model (fine to use caret). Run the `class_diag` function or equivalent to get out-of-sample performance averaged across your k folds.
k = 10

data <- Medicaid[sample(nrow(Medicaid)), ]
folds <- cut(seq(1:nrow(Medicaid)), breaks= k, labels= F) 

diags <- NULL
for (i in 1:k) {
    #creates training and test sets
    train <- data[folds != i,]
    test <- data[folds == i,]
    truth <- test$enrollbi
    
    # test model on test set
    fit <- knn3(enrollbi ~ ., data= train)
    probs <- predict(fit, newdata= test)[,2]
    
    #diagnostics for fold
    diags <- rbind(diags, class_diag(probs, truth, positive = 1))
}

summarize_all(diags, mean)

```

The accuracy of my knn was 0.479, meaning that proportion of participants in the dataset were correctly classified as enrolled in a demonstration program through Medicaid based on the variables visits, children, age, health1, and access. The sensitivity was calculated to be 0.4873, meaning roughly 48% of the positive reports from the data were actually positive. Finally, the auc value shows that this model preformed significantly worse compared to the linear classifier section, with a value of 0.48203. 


### Regression/Numeric Prediction

```{R}
# regression model code here
library(rpart)
library(rpart.plot)
# Fit a linear regression model or regression tree to your entire dataset, predicting one of your numeric variables from at least 2 other variables


tree <- train(income ~., data = Medicaid, method = "rpart")
rpart.plot(tree$finalModel)
#Report the MSE for the overall dataset
#mean squared error
model <- lm(income ~ visits*access, data=Medicaid)
yhat <- predict(model)
mean((Medicaid$income-yhat)^2)

```

```{R}
# cross-validation of regression model here

#Perform k-fold CV on this same model (fine to use caret). Calculate the average MSE across your k testing folds.
k = 10

data <- Medicaid[sample(nrow(Medicaid)), ]
folds <- cut(seq(1:nrow(Medicaid)), breaks= k, labels= F) 

diags <- NULL
for (i in 1:k) {
    train <- data[folds != i,]
    test <- data[folds == i,]
    
    # fits linear regression model 
    # predictions and y-hat on test set
    fit <- lm(income ~ ., data= train)
    yhat <- predict(fit, newdata= test)
    
    #MSE across k testing folds
    diags <- mean((test$income-yhat)^2)
}

mean(diags)

```
The mean squared error for the data set was reported at a very high 13.12154. The MSE across k testing folds was slightly lower at 12.47822. Both of these values indicate that measure of prediction error was high, and shows signs of overfitting in both models. 

### Python 

```{R}
library(reticulate)
```

```{python}
# python code here
# Include a python code chunk in your project

# Using `reticulate`, demonstrate how you can share objects between R and python using `r.` and `py$`

# Include a sentence or two describing what was done

```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




